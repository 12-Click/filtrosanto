<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Saint Francis Xavier Face Lens</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            overflow: hidden;
            height: 100%;
        }

        .container {
            position: relative;
            height: 100%;
            width: 100%;
        }

        #video {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }

        .overlay {
            position: absolute;
            width: 200px; /* Adjust for the halo size */
            height: 200px; /* Adjust for the halo size */
            background: url('halo.png') no-repeat center center; /* Use your halo image */
            background-size: cover; /* Adjust to fit */
            pointer-events: none; /* So it doesn't block the video */
            transform: translate(-50%, -50%); /* Center the overlay */
        }
    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tensorflow/3.9.0/tensorflow.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>
</head>
<body>
    <div class="container">
        <video id="video" autoplay playsinline></video>
        <canvas id="canvas"></canvas>
        <div class="overlay" id="halo"></div>
    </div>
    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const halo = document.getElementById('halo');
        const ctx = canvas.getContext('2d');

        // Set up the video stream
        navigator.mediaDevices.getUserMedia({ video: true })
            .then(stream => {
                video.srcObject = stream;
            })
            .catch(err => {
                console.error('Error accessing the camera: ', err);
            });

        // Load the face mesh model
        async function loadFaceMesh() {
            const model = await faceLandmarksDetection.load(faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);
            detectFaces(model);
        }

        // Detect faces and update the overlay position
        async function detectFaces(model) {
            while (true) {
                const predictions = await model.estimateFaces({ input: video });

                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                if (predictions.length > 0) {
                    const face = predictions[0];
                    const x = (face.boundingBox.topLeft[0] + face.boundingBox.bottomRight[0]) / 2;
                    const y = (face.boundingBox.topLeft[1] + face.boundingBox.bottomRight[1]) / 2;

                    // Position the halo slightly above the detected face
                    halo.style.left = `${x}px`;
                    halo.style.top = `${y - 100}px`; // Adjust 100 to position it higher above the face
                } else {
                    halo.style.left = `50%`; // Reset position if no face detected
                    halo.style.top = `50%`; // Reset position if no face detected
                }

                await new Promise(requestAnimationFrame); // Use requestAnimationFrame for smoothness
            }
        }

        video.addEventListener('loadedmetadata', () => {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            loadFaceMesh();
        });
    </script>
</body>
</html>

